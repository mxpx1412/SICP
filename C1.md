# 1 Building Abstractions with Procedures

+ *Computational process*: abstractions in computers that manipulate 
other abstractions called *data*.
+ *Program* directs pattern of rules in a computer process.
+ *Programming languages* are series of expressions used to compose 
programs, prescribing the programs' tasks.
+ *Bugs* and *glitches* are errors in programs, which can have 
complex and unanticipated consequences.
+ Good software engineers anticipate and prevent errors.
+ *Debugging* is done when errors occur in programs.
+ *Modularity* allows separate construction, replacement and debugging
 of programs.
+ *Interpreter* is a machine that carries out tasks described by 
programming language.

## Programming in Lisp

+ SICP uses *'Scheme' dialect* of the *Lisp* programming language.
+ Lisp was developed by John McCarthy for the reasoning of *recursive 
equations*.
+ Lisp *procedures* can also be manipulated as Lisp data.

## 1.1 The Elements of Programming

### Mechanisms of Languages to Create Complexity From Simple Ideas

+ **Primitive expressions**: simplest entities of the language.
+ **Means of combination**: way to build compound elements from 
simpler ones.
+ **Means of abstraction**: way for compound elements to be named 
and manipulated as units

### Elements of Programming Language

+ **Data**: "stuff" to be manipulated
+ **Procedures**: descriptions of rules for manipulating data
+ Powerful programming language be able to describe primitive 
data, procedures; and have ways to combine and abstract procedures 
and data.

### 1.1.1 Expressions

+ *Expressions* are typed into terminals (goes through interpreter) 
and then expressions are *evaluated*.
+ Numbers are examples of primitive data expressions in Lisp.
+ Addition, substraction (i.e. `+`, `-`) etc are examples of 
primitive procedure expressions in Lisp.
+ *Combinations* are delimited list of expressions in parentheses 
that denote procedure application.
+ Example of combination:

```scheme
(+ 1 1)
;Value: 2
```

+ *Operator*: specifier of the procedure (e.g. the `+` in above).
+ *Operand*: elements on which the operator's procedure is applied.
+ *Arguments*: values of the operands.
+ Value of combination obtained by applying operator's procedure to 
the arguments.
+ *Prefix notation*: convention to place operator left of the 
operands.
+ *Nesting*: have combinations whose elements are also combinations.
+ Nesting example:

```scheme
(+ (* 2 3) (- 7 1))
```

+ *Pretty printing*: formatting convention where each long 
combination written with vertically aligned operands.
+ *Read-eval-print loop*: interpreter operates by reading expression 
from terminal, evaluating the expression then printing result.

### 1.1.2 Naming and Convention 

+ *Variable*: object represented by a name. *Value* of the variable 
is the object represented.
+ Define variable in Lisp (name `size` with value `2`):

```scheme
(define size 2)
```

+ Complex programs are built up from simple parts with increasing 
levels of abstraction.
+ *Global environment*: *memory* kept to remember name-object pairs

### 1.1.3 Evaluating Combinations

+ *Evaluation rule for Scheme* (applicative order evaluation):

  1. Evaluate subexpressions of combination.
  2. Apply leftmost operator to arguments (which are values 
  of the other subexpressions (the operands)).

+ Evaluation is *recursive*, invoking itself.
+ *Recursion* (processes invoking itself) useful for hierarchical, 
tree-like objects. 
+ Evaluating tree representation "upwards" called *tree accumulation*
+ After first step, only primitive expressions need to be evaluated 
not combinations.
+ Primitive cases resolved by setting:

  1. Values of numerals are numbers named.
  1. Value of built in operators are machine-instructed 
  sequence carrying corresponding operations.
  1. Values of other names are objects with same name in 
  environment.

+ Environment provides context for evaluation to take place.
+ *Special forms*: expressions such as `define` that does not follow 
above evaluation rule.
+ *Syntax* of programming language dictated by kinds of expression 
and their evaluation rules in the language.

### 1.1.4 Compound Procedures

+ *Procedure definition*: abstraction technique where compound 
operation is named and referred to as a unit.
+ Procedure definition example:

```scheme
(define (square x) (* x x))
```

+ General form of procedure definition:

```scheme
(define (name formalParameters) (body))
```

+ The `name` is symbol to associate w/ procedure def in environment.
+ The `formalParameters` are names used in the procedure body to 
refer to arguments of the procedure
+ The `body` is an expression that will yield final value of 
procedure when actual arguments are given to place of formal params.

### 1.1.5 The Substitution Model for Procedure Application

+ *Substitution model* for applying procedures: evaluate body of the 
procedure with each formal parameter replaced by corresponding arg.
+ Note substitution model does not describe actual workings of 
interpreter, and is only one of many models for an interpreter

#### Applicative order versus normal order

+ *Applicative order*: evaluates the operator and operands then 
apply resulting procedures to resulting arguments.
+ *Normal order*: substitute operands for parameters until 
only primitive expressions are left, then evaluate.
+ Normal order example:

```scheme
(sum-of-squares (+ 5 1) (* 5 2)
(+ (square (+ 5 1)) (square (* 5 2)))
(+ (* (+ 5 1) (+ 5 1)) (* (* 5 2) (* 5 2)))
(+ (* 6 6) (* 10 10))
(+ 36 100)
136
```
+ For procedures that can be modelled by substitution and yield 
legitimate values, normal-order and applicative order evaluation 
produces the same result.
+ Lisp uses applicative-order evaluation.

### 1.1.6 Conditional Expressions and Predicates

+ *Case analysis* where several cases are tested and the value is 
determined by case is done by `cond` in scheme.
+ `cond` example (absolute value function):

```scheme
(define (abs x)
  (cond ((> x 0) x)
  ((= x 0) 0)
  ((< x 0) (- x))))
```

+ General form: series of *clauses* with first expression being a 
*predicate* (expression inerpreted to either true or false).
+ Conditionals evaluated by testing until the first predicate, then 
returning value of corresponding *consequent expression*.
+ Symbol `else` can be used in final clause of `cond` to return a 
value if all other predicates bypassed.
+ Special form `if` may be used if only two cases in case analysis

```scheme
(if predicate consequent alternative)
```

+ For `if`: predicate is evaluated, giving consequent value if 
predicate true, else gives alternative.
+ Primitive predicates: `>`, `<`, `=`.
+ `(and e_1 e_2 e_n)`: evaluates expression one at a time, returning 
true if all expressions true, if any expression is false, gives false.
+ `(or e_1 e_2 e_n)`: evaluates expression one at a time, if any is 
true return true, if all are false return false.
+ `(not e)`: is true if `e` false, is false if `e` true.
+ `and` and `or` are special forms.
+ `not` is ordinary procedure.

### 1.1.7 Example: Square Roots by Newton's Method

+ Difference between computer procedure vs. mathematical function: 
procedure must be effective.
+ i.e. *imperative knowledge* (how-to) vs. *declarative knowledge* 
(what-is).
+ *Newton's method for square roots*: for number $x$, guess value 
$y$ as root, then iteratively compute average between $y$ and $x/y$, 
using the average as the new $y$ each time.
+ [Can be implemented in scheme](./C1_1_7.scm)
```scheme
(define (good-enough-sqrt guess x) 
  (< (abs (- (square guess) x)) 0.001))

(define (average y z) 
  (/ (+ y z) 2))

(define (improve-sqrt guess x) 
  (average guess (/ x guess)))

(define (sqrt-iter guess x) 
  (if (good-enough-sqrt guess x) 
        guess 
        (sqrt-iter (improve-sqrt guess x) x)))

(define (sqrt x) (sqrt-iter 1 x))
```
+ Implementation shows iterations can be accomplished by ordinary 
procedure involving recursion.

### 1.1.8 Procedures as Black-Box Abstractions

+ Recall *"recursion"*: procedure defined in terms of itself.
+ Note that `sqrt` is a procedure consisting of a cluster of 
sub-procedures, mirroring the decomposition of the problem 
into subproblems. 
+ Each sub-procedure is able to treat the other sub-procedures 
as "black-boxes" i.e. *procedural abstractions*
+ E.g. the procedure to iterate finding the square root does 
not need to know how the procedure for determining if guess 
is good enough works.
+ Procedure definition should be able to suppress detail. 
Users should not need to know implementation to use a 
procedure.

#### Local names

+ The names for the procedure's formal parameters should 
not matter to the user.
+ Consequently parameter names of a procedure must be 
local to the body of the procedure.
+ *Bound variable*: formal parameter whose name does not 
matter. Consistently renaming a bound variable in a 
procedure definition will not change the definition.
+ *Free variable*: variables that are not bound.
+ *Scope* of a name: set of expressions for which a 
binding defines a name. 
+ In procedure definition, the bound variables declared 
as formal parameters of the procedure has the procedure 
asthe scope.
+ Renaming a bound variable to name of a free variable 
(e.g. `<`, `-`, `abs`) will cause error by *capturing* 
the variable.
+ Procedures are not independent of the names of its 
free variables.

#### Internal definitions and block structure

+ Sub-procedures can be defined within another procedure 
to make them local.
+ *Block structure*: nesting of definitions with local 
subprocedures inside other procedure.
+ E.g. `improve-sqrt` and `good-enough-sqrt` can be 
internally defined within `sqrt`.
+ Procedure can be simplified by making variables common 
to multiple internal definitions free variables. 
+ E.g. the `x` can be a free variable in the scope of 
`sqrt` throughout the various internal definitions of 
`improve-sqrt`, `good-enough-sqrt` etc.
+ *Lexical scoping*: free variables in a procedure are 
looked up in the environment in which the procedure was 
defined. I.e. they refer to bindings made by enclosing 
procedures.

## 1.2 Procedures and the Process They Generate

+ Procedure is pattern for *local evolution* of a compuational 
process.
+ The overall *global* behaviour of a proces with local evolution 
specified by a procedure is of interest, but difficult to 
determine in general.
+ General "shapes" for processes generated by simple procedures 
exist.

### 1.2.1 Linear Recursion and Iteration

+ Consider factorial where $n\! = \prod\limits_{i=1}^n i$.
+ [Recursive procedure for factorial.](./Book_Examples_Source/C1_2_1a.scm)
```scheme
(define (factorial n) 
  (if (= n 1) 
    1
    (* n (factorial (- n 1)))))
```
+ Procedure for factorial can also be specified by implementing a 
counter that successively multiply numbers until $n$ is reached 
in counter.
+ [Iterative procedure for factorial.](./Book_Examples_Source/C1_2_1b.scm)
```scheme
(define (fact-iter product counter max-count) 
  (if (> max-count counter) 
    product 
    (fact-iter (* counter product) 
               (+ counter 1) 
               max-count)))

(define (factorial n) 
  (fact-iter 1 1 n))
```
+ First procedure has "expand-then-contract shape" when described 
with substitution model.
+ Expansion from *deferred operations*, then contraction from 
operations being actually performed. 
+ *Recursive process*: process characterized by chain of deferred 
operations. 
+ *Linearly recursive process*: recursive processes where the chain 
of deferred operations grows linearly with input (e.g. in $n!$ grows 
linearly with $n$). 
+ Second procedure does not grow and shrink, only need to keep track 
of current state of variables `product`, `counter`, `max-count`.
+ *Iterative process*: processes whose state can be summarized by 
fixed number of *state variables* and a fixed rule on how state 
variables are updated, plus optionally a test to determine when 
process terminates.
+ *Linear iterative process*: iterative steps grows linearly with 
input.
+ In iterative process, program variables describe complete 
description of process state at any given point.
+ In recursive process, additional hidden information is maintained 
by the interpreter, uncontained in program variables.
+ *Recursive process* vs. *recursive procedure*: when *procedure* 
is recursive, it means procedure is defined in terms of itself; 
when *process* is recursive, it means process follow an evolution 
pattern with chain of deferred operations.
+ Note `fact-iter` in [example](./Book_Examples_Source/C1_2_1b.scm) 
is an iterative process that is also a recursive procedure.
+ Note in many implementations of languages recursive procedures 
(even if iterative) consume growing memory with more procedure 
calls. As such iteration is done using looping-constructs, and 
descriptively iterative process that are recursive procedures 
resembles resursive processes more in memory use.
+ *Tail-recursive*: iterative processes described by recursive 
procedure, but only executes iterative process in constant space 
in implementation.

### 1.2.2 Tree Recursion

+ The computation of fibonacci numbers as defined by a 
recursive procedure creates a tree pattern.
+ $\mathrm{Fib}(0)=0$, $\mathrm{Fib}(1)=1$, 
$n > 1 \implies \mathrm{Fib}(n)=\mathrm{Fib}(n-1)+\mathrm{Fib}(n-2)$
+ [Recursive process `fib`](./Book_Examples_Source/C1_2_2a.scm)
```scheme
(define (fib n) 
  (cond ((= n 0) 0)
        ((= n 1) 1)
        (else (+ (fib (- n 1)) 
                 (fib (- n 2))))))
```
+ Tree pattern splits to two per level since `fib` calls itself 
twice per invocation.
+ Note $\mathrm{Fib}(n)$ is the closest integer to 
$\phi^n/\sqrt{5}$, where $\phi=\frac{1+\sqrt{5}}{2}$ 
is the *golden ratio* satisfying $\phi^2=\phi+1$.
+ Steps in process defined for $\mathrm{Fib}(n)$ grows 
exponentially with input. Space required grows linearly with 
input.
+ Process is a *tree recursion*.
+ Alternative [iterative process `fib`](./Book_Examples_Source/C1_2_2b.scm)
```scheme
(define (fib-iter a b count) 
  (if (= 0 count) 
    b
    (fib-iter (+ a b) a (- count 1))))

(define (fib n) 
  (fib-iter 1 0 n))
```
+ Second iterative process is a linear iteration. Steps grow 
linearly with $n$. 
+ Even though second iterative process much more efficient, first 
recursive process more natural w.r.t. the problem.

#### Example: Counting Change

+ Challenge: compute number of ways change can be given for some 
amount of money, change will be given in half-dollars, quarters, 
dimes, nickels and pennies.
+ The number of ways to change amount $a$ using $n$ kinds of coins: 
  + ... is number of ways to change amount $a$ using all but 
  the first kind of coin, plus
  + ... the number of ways to change amount $a-d$ using all 
  $n$ kinds of coins, where $d$ is the denomination fo the 
  first kind of coin.
  + If $a=0$, count as $1$ way to make change.
  + If $a<0$, count as $0$ way to make change.
  + If $n=0$, count as $0$ ways to make change.
+ Procedure can be translated into tree-recursive process. Easily 
understood in tree-recursive form but inefficient. 

### 1.2.3 Orders of Growth

+ *Order of growth*: measure of resources required by process 
as inputs increase.
+ Given $n$ input size, let $R(n)$ be resources required for 
given $n$ inputs. Then we say $R(n)$ has order of growth 
$\Theta(f(n))$ and $R(n) = \Theta(f(n))$ if there are positive 
constants $k_1$ and $k_2$ such that 
$k_1 f(n) \leq R(n) \leq k_2 f(n)$ for any sufficiently large 
value of $n$. (i.e. $R(n)$ bound by $k_1 f(n)$ and 
$k_2 f(n)$ for sufficiently large $n$).
+ E.g. linear recursive factorial process: $\Theta(n)$ steps 
required, and $\Theta(n)$ space required.
+ E.g. Iterative factorial process: $\Theta(n)$ steps required, 
but $\Theta(1)$ space required. 
+ E.g. Tree-recursive Fibonacci process: $\Theta(\phi^n)$ steps 
and $\Theta(n)$ space required. 

### 1.2.4 Exponentiation

+ Recursive definition for exponentiation:
  + $n\in\mathbb{Z}$, $n\gt 0 \implies b^n = b\cdot b^{n-1}$
  + $b^0 = 1$
+ Observe *successive squaring*:

$$
\begin{align}
b^2 &= b\cdot b \\
b^4 &= b^2 \cdot b^2 \\
b^8 &= b^4 \cdot b^4 
\end{align}
$$

+ Additional observation on exponentiation:
  + $n$ even then $b^n = \left(b^{n/2}\right)^2$
  + $n$ odd then $b^n = b\cdot b^{n-1}$
+ If successive squaring used to define exponentiation in a recursive 
process, the process evolved grows only in $\Theta(\mathrm{log}(n))$ in 
both space and time. 
+ Without successive squaring, the exponentiation grows in $\Theta(n)$ 
in space and steps for recursive process example given, and in $\Theta(1)$ 
for space and $\Theta(n)$ in steps for linear process example given.

### 1.2.5 Greatest Common Divisors

+ The *greatest common divisor* (GCD) of two integers $a$ and $b$ is the 
largest integer that divides $a$ and $b$ with no remainder.
+ **Observation**: let $r:=\mathrm{remainder}\left(a, b\right)$, then we have:
  $$\mathrm{GCD}(a, b) = \mathrm{GCD}(b, r)$$

#### Euclid's Algorithm

In general if $r:=\mathrm{remainder}\left(a,b\right)$, 
then we have $\mathrm{GCD}(a, b)=\mathrm{GCD}(b, r)$. Then let $a_0 := a$, 
$b_0 := b$; also let $r_i := \mathrm{remainder}\left(a_i, b_i\right)$, 
$a_i = b_{i-1}$, and $b_i = r_{i-1}$. We see that:

$$
\begin{align}
\mathrm{GCD}(a,b) &= \mathrm{GCD}(a_0, b_0) \\
  &= \mathrm{GCD}(a_1, b_1) \\
  &= \dots \\
  &= \mathrm{GCD}(a_i, b_i) \\
  &= \dots \\
\end{align}
$$

Then for some $n \in \mathbb{Z}$, we have 
$\mathrm{GCD}(a_n, b_n)=\mathrm{GCD}(a_n,0) = a_n$, and 
$a_n \in \mathbb{Z}$ is precisely $\mathrm{GCD}(a, b)$. 

+ [Following procedure](./Book_Examples_Source/C1_2_5.scm) implements Euclid's 
algorithm. The iterative process generated has steps growing as logarithm of 
the numbers involved.
```scheme
(define (gcd a b) 
    (if (= b 0) 
        a
        (gcd b (remainder a b))))
```

#### Lamé's Theorem

If Euclid's Algorithm requires $k$ steps to compute the 
$\mathrm{GCD}$ of some pair, then the smaller number in the pair must be 
greater than or equal to the $k^\mathrm{th}$ Fibonacci number.

+ By Lamé's theorem, we see that if $n$ is the smaller of $a$, $b$ in 
$\mathrm{GCD}(a,b)$, we have $n\geq \mathrm{Fib}(k)\approx \varphi^k/\sqrt{5}$ 
and thus the number of steps $k$ has order of growth $\Theta(\log n)$.

### 1.2.6 Example: Testing for Primality

#### Searching for Divsiors

Prime testing can be done by [searching for divisors](./Book_Examples_Source/C1_2_6a.scm), 
using the fact that if $n$ is not prime, it should have a divisor smaller 
or equal to $\sqrt{n}$. Resulting steps will have order of growth 
$\Theta(\sqrt{n})$. 
```scheme
; Searching for divisors

(define (smallest-divisor n) (find-divisor n 2))

(define (find-divisor n test-divisor) 
    (cond 
        ((> (square test-divisor) n) n) 
        ((can-divide test-divisor n) test-divisor)
        (else (find-divisor n (+ test-divisor 1)))))

(define (can-divide a b) (= (remainder b a) 0))

(define (is-prime n) 
    (= n (smallest-divisor n)))
```

#### The Fermat Test

+ *Congruent modulo*: two numbers $a$ and $b$ are "congruent modulo $m$" 
(for some $m\in \mathbb{Z}$, $m\neq 0$) 
if there exists integer $k$ such that $a - b = km$, and we write 
$a \equiv b \pmod{m}$. This is equivalent to saying $a$ and $b$ have the 
same remainder when divided by $m$.
+ Notation: we will write $(a \mod m)$ OR $(a\,\mathrm{mod}\,m)$ for "the remainder 
of $a$ when divided by $m$".

##### Fermat's Little Theorem

If $n$ is prime, then for positive integer $a$ where $a \lt n$, 
$a^n$ is congruent to $a$ modulo $n$ (in other words, 
$\exists q\in\mathbb{Z}$ such that $a^n - a = qn$).

+ Fermat's little theorem can be used to write [probablistic procedure for 
testing primality](./Book_Examples_Source/C1_2_6b.scm).
```scheme
(define (expmod base expnt m) 
    (cond 
        ((= expnt 0) 1) 
        ((is-even expnt) 
            (remainder 
                (square (expmod base (/ expnt 2) m)) 
                m))
        (else 
            (remainder 
                (* base (expmod base (- expnt 1) m))
                m))))

(define (fermat-test n) 
    (define (fermat-trial a) 
        (= (expmod a n n) a))
    (fermat-trial (+ 1 (random (- n 1)))))

(define (fast-prime-fermat n times) 
    (cond 
        ((= times 0) true)
        ((fermat-test n) (fast-prime-fermat n (- times 1)))
        (else false)))
```
+ The `expmod` procedure in the Fermat test example makes use of properties 
of modular arithmetic, particularly that it is compatible with multiplication.
  + We see that $a\equiv (a\,\mathrm{mod}\,m) \pmod{m}$ by reflexivity. So:
  $$a\,\mathrm{mod}\,m=(a\,\mathrm{mod}\,m)\,\mathrm{mod}\,m$$
  + By compatibility of congruent modulo $n$ with multiplication: if 
  $a \equiv b \pmod{m}$ and $c \equiv d \pmod{m}$, then 
  $ac \equiv bd \pmod{m}$.
    + Choose $a:=a$, $b:=(a\,\mathrm{mod}\, m)$, $c:=a^{n-1}$ and 
    $d:=\left(\left(a^{n-1}\right) \,\mathrm{mod}\, m\right)$:
    $$
    \implies \left(a^n\,\mathrm{mod}\, m\right) 
      = \left(\left(a\,\mathrm{mod}\, m\right)\cdot\left((a^{n-1})\,\mathrm{mod}\, m\right)\right)\,\mathrm{mod}\, m
    $$
    + Similarly if we choose $a^n$:
    $$
    \left(a^{2n}\,\mathrm{mod}\, m\right) 
      = \left( \left(a^n \,\mathrm{mod}\, m\right) \cdot \left(a^n \,\mathrm{mod}\, m\right)\right) \,\mathrm{mod}\, m
    $$
  + See [additional notes](Book_Examples_Source/C1_2_6_modulo.md) 
  for detailed properties of congruent modulo $n$.
+ Successive squaring by `expmod` gives $\Theta(\log n)$ order of 
growth in steps for the Fermat test procedure shown.

#### Probabilistic Methods

+ The Fermat test is probabilistic, unlike typical algorithms guaranteed to 
give correct answer.
+ Failing Fermat test means $n$ definitely not prime, but passing does not 
guarantee $n$ to be prime. 
+ Some numbers are non-prime *and* passes Fermat test every time. Such 
numbers rare in practice, so Fermat test usually reliable.
  + These are called the *Carmichael numbers*.
  + Smallest few Carmichael numbers: 561, 1105, 1729, 2465, 2821, 6601.
+ Combining Fermat method with additional test allows error to 
be made as small as desired.
+ *Probabilistic algorithms* have particular use for tests where chance of 
error becomes arbitrarily small.

##### Miller-Rabin Test (From Exercise 1.28)

+ *Alternate form of Fermat's Little Theorem*: if $n$ is prime and 
$a\lt n$ is a positive integer, then $a^{n-1} \equiv 1 \pmod{n}$. 
+ *Miller-Rabin test* is a variant of Fermat test that returns correct 
result for the Carmichael numbers:
  + For a positive integer $a\lt n$, compute $a^{n-1}\mod n$. 
  + During the computation of $a^{n-1}\mod n$, see if a "non-trivial 
  square root of $1$ modulo $n$" is found. If such a square root 
  is found, then $n$ is not prime.
  + For all non-prime odd numbers, then at least half the numbers 
  $a\lt n$, computing $a^{n-1}$ in the manner of `expmod` will 
  reveal a nontrivial square root of $1\mod n$. 
  + But if no such nontrivial square root is found, and we found 
  $a^{n-1} \equiv 1\pmod{n}$ then, we can say that $n$ is prime. 
+ Note regarding the "roots" being searched:
  + "Nontrivial square root of $1$ modulo $n$" does *NOT* mean a number 
  $y$ where $y^2 = 1\mod n$. 
  + The statement is rather referencing *equivalent classes*.
  + *The equivalence class of $a \mod m$* is the set of all integers that 
  are congruent to $a$, $\mod m$. I.e. it is the set:
  $$[a]_m:=\{x:\,x\in\mathbb{Z},\,x\equiv a\pmod{m}\}$$
  + So in fact, "nontrivial square root of $1$ modulo $n$" is the set: 
  $$S:=\{y:\,y\in\mathbb{Z},\,y^2\equiv 1\pmod{n}\}$$
  + Therefore in the Miller-Rabin test, what is being searched for are cases 
  where after the squaring procedure, the result is congruent to $1$, 
  modulo $n$, i.e. numbers that have remainder $1$ when dividied by $n$. 

## 1.3 Formulating Abstractions with Higher Order Procedures

+ Powerful programming languages should have ways to build abstractions by 
assigning names to a common pattern and allow the abstraction to be applied 
directly.
  + E.g. instead of performing `(* x x)` in terms of only primitives, 
  it's useful to build abstraction by `(define (square x) (* x x))`.
  + Most languages therefore allow procedural definition.
+ *Higher-order procedures*: procedures that manipulate procedures. 
+ Same programming pattern often applicable to different procedures, 
so higher-order procedure allows for increased abstraction.

### 1.3.1 Procedures as Arguments

+ All three of the following contain a same underlying concept: 
  + Sum of integers from $a$ to $b$:
  $$\sum_{i=a}^b i$$
  + Sum of the cube of integers of $a$ to $b$:
  $$\sum_{i=a}^b i^3$$
  + Sum of a particular sequence of terms from a series by Leibniz:
  $$\sum_{i=a}^b \frac{1}{(1+4i)(1+4i+2)}$$
    + Series: $\frac{1}{1\cdot 3}+\frac{1}{5\cdot 7}+\frac{1}{9\cdot 11}+\dots$
  + **Common concept for above sums**: take integers $a$ and $b$, 
  $a\lt b$. Map $a$ to some new number and add to a growing sum. 
  Increment to the next integer until we reach $b$.
  + *Summation of a series* is the common concept: 
  $$\sum_{i=a}^b f(i) = f(a) + \dots + f(b)$$
  + Using $\sum$ notation allows study of summation concept directly. 
  Similar abstractions available for programming.
  + [When the sums are defined as procedures](./Book_Examples_Source/C1_3_1.scm), 
  we see that the only differences are the name of the procedures, the 
  function to compute the next summed term, and the function to 
  increment to the next integer. 
    + Subsequently, can define `sum` procedure that take other 
    procedures `sum-term` and `next-term` as arguments. 
    + By passing `cube` as `sum-term` and `inc` as `next-term`, the procedure 
    will behave same as `sum-cubes`.  
    + Also works for `sum-integers` and `pi-sum` if proper procedures passed 
    to `sum`.
  ```scheme
  (define (sum sum-term a next-term b) 
    (if 
      (> a b) 
      0 
      (+ 
        (sum-term a) 
        (sum sum-term (next-term a) next-term b))))
  ```
  + Note *definite integral of $f$ between $a$ and $b$* approximately: 
  $$
    \int_{a}^{b} f(x) \,\mathrm{d}x
    \approx \left[ 
      f\left(a + \frac{\mathrm{d}x}{2}\right) 
      + f\left(a + \mathrm{d}x + \frac{\mathrm{d}x}{2}\right) 
      + f\left(a + 2\mathrm{d}x + \frac{\mathrm{d}x}{2}\right) 
      + \dots
      \right] \cdot \mathrm{d}x
  $$
    + Can use `sum` to define as procedure. 
+ *Accumulator* is the generalized notion for `sum` and `product`, where 
a collection of terms are combined by some *combiner* (see 
[exercise 1.32](./Exercise_Source/E1_32a.scm)).
+ *Filter* allows for even greater generalization, where only terms allowed 
by the filter is processed (see example of *filtered accumulator* in 
[exercise 1.33](./Exercise_Source/E1_33.scm)).

### 1.3.2 Constructing Procedures Using `lambda`

+ Previously, some procedures for computing `next-term` and `sum-term` in 
`sum` are trivial. Explicitly defining such procedures is tedious. 
+ `lambda` is a special form that creates procedures:
  + E.g. `lambda` that increments input by `dx`:
  ```scheme
  (lambda (x) (+ x dx)) 
  ```
    + "The procedure of an argument `x` that adds `x` and `dx`
  + Re-writing [`integral-mid-pt`](./Exercise_Source/E1_30.scm) in terms 
  of `lambda`:
  ```scheme
  (define (integral-mid-pt f a b dx) 
    (* (sum f (+ a (/ dx 2.0)) (lambda (x) (+ x dx)) b) dx))
  ```
  + Generally `lambda` creates procedures similar to `define`, except the 
  resulting procedure is un-named (annonymous) in the environment: 
  ```scheme
  (lambda (<formal-params>) <body>)
  ```
  + `lambda` can be used in place of procedural names:
    + Consider following: 
    ```scheme
    (define (square x) (* x x))
    (square 2) 
    ```
    + ... can be rewritten as: 
    ```scheme
    ((lambda (x) (* x x)) 2) 
    ```
  + "*Maps to*" notation $x\mapsto f(x)$ is mathematical way to express 
  `lambda`.

#### Using `let` to create local variables

+ `lambda` can be used to create *local variables* (variables other than 
those bound as formal parameters). 
+ Consider:
  $$f(x,y) = x(1 + xy)^2 + y(1-y) + (1 + xy)(1 - y)$$
  + Can be rewritten in terms: 

$$
\begin{align}
a &= 1 + xy \\
b &= 1 - y \\
f(x, y) &= xa^2 + yb^2 + ab 
\end{align}
$$

  + [Can be defined as procedure in several ways](./Book_Examples_Source/C1_3_2.scm), 
  including using the special form `let`.
+ `let` is a special form that shortens local variable assignments further: 
```scheme
(let ((<var-1> <exp-1>) 
      (<var-2> <exp-2>) 
      <...>
      (<var-i> <exp-i>) 
      <...>
      (<var-n> <exp-n>)) 
  <body>)
```
  + Meaning: "let `var-i` have value `exp-i` in the body".
  + When `let` evaluated, each name associated with value of corresponding 
  expression. Then body evaluated with names bound as local variables. 
  + `let` is an alternate syntax (i.e. syntactic sugar) for: 
  ```scheme
  ((lambda 
    (<var-1> <var-2> <...> <var-i> <..> <var-n>) 
    <body>) 
    <exp-1> <exp-2> <...> <exp-i> <...> <exp-n>)
  ```
  + Scope of variable specified by `let` is the `<body>` in `let`. This means 
  `let` allows binding of variables "as locally as possible".
    + By scope property, the final result is `5` in the following: 
    ```scheme
    (define x 1)
    (+ (let ((x 2)) (square x)) x)
    ;Value: 5
    ```
    + The values of local variables are computed outside `let`. So if a local 
    variable uses a name present as both another local variable AND a variable 
    outside `let`, the value of outside `let` will be used. E.g. following 
    results in value `6`, local variable `y` has value `4` when `(+ x y)` is 
    evaluated: 
    ```scheme
    (define x 1) 
    (let ((x 2) (y (+ x 3))) (+ x y))
    ;Value: 6
    ```
+ Internal definitions may produce same result as `let`, but it is preferred 
to use internal `define` only for internal procedures (this concerns 
evaluation, will be discussed later). 

### 1.3.3 Procedures as General Methods

+ Procedures used to describe general methods of computation are powerful 
abstractions. 

#### Finding roots of equations by the half-interval method

+ *Half-interval method*: for continuous function $f(x)$, 
if we have $a$, $b$ such that $f(a) \lt 0 \lt f(b)$, then 
there must exist $x\in (a,b)$ where $f(x) = 0$, so we can 
find the root of $f$. 
  + With initial $a_0$, $b_0$, choose $x_0 = \mathrm{average}(a_0,b_0)$. 
  + If $f(a_0)\lt 0 \lt f(x_0)$, then choose $a_1 = a_0$, $b_1 = x_0$ 
  and iterate. Else if $f(x_0)\lt 0 \lt f(b_0)$, then choose $a_1 = x_0$, 
  $b_1 = b_0$ and iterate. 
  + As $i\to \infty$ for $f(x_i)\to 0$.
  + Uncertainty halves per step of process, therefore steps grow as 
  $\Theta(\log(L/T))$, where $L$ is length of initial interval and $T$ 
  is error tolerance interval. 
+ [Half-interval method in scheme](./Book_Examples_Source/C1_3_3a.scm).
```scheme
(define (search-root f neg-point pos-point) 
  (let ((midpoint (average neg-point pos-point))) 
  (if 
    (close-enough neg-point pos-point) 
    midpoint 
    (let ((test-value (f midpoint))) 
      (cond 
        ((> test-value 0) (search-root f neg-point midpoint)) 
        ((< test-value 0) (search-root f midpoint pos-point)) 
        (else midpoint))))))

(define (half-interval-method f a b) 
  (let 
    ((a-val (f a)) (b-val (f b))) 
    (cond 
      ((and (negative? a-val) (positive? b-val)) 
        (search-root f a b)) 
      ((and (negative? b-val) (positive? a-val)) 
        (search-root f b a)) 
      (else 
        (error "Values are not of opposite sign!" a b)))))

```

### Finding fixed points of functions 

+ A *fixed point* of a function $f$ is $x$ such that $f(x)=x$. 
+ For **some** functions $f$, can find fixed point by iteration:
$$ 
\lim_{n\to\infty} \left(f^n(x) - f^{n-1}(x)\right) = 0
$$
+ Similar to finding square root method from before, in fact: 
  + If $y^2 = x$, then re-write as $y = \frac{x}{y}$, we see 
  that this is a fixed-point problem for function $y \mapsto x/y$. 
  + However, a simple mapping as above does NOT converge, guess results 
  in infinite loop: 

$$
\begin{align}
&y_1 \\
y_2 &= \frac{x}{y_1} \\
y_3 &= \frac{x}{\left(\frac{x}{y_1}\right)} = y_1 \\
&\dots
\end{align}
$$

  + But observe: $\mathrm{ans}\in [y, x/y]$, therefore can choose guess 
  in-between, such as the average of $y$, $x/y$. 
    + This is a simple transformation: 

$$
\begin{align}
y &= \frac{x}{y} \\
2y &= \frac{x}{y} + y \\
y &= \frac{1}{2}\left(\frac{x}{y} + y\right)
\end{align}
$$

    + "True answer is between any particular guess $y$ and $x/y$": this 
    statement can be proved by considering $y_t+\epsilon$ with
    $\epsilon\gt 0$ and $y_t$ being true square root of $x$. Then 
    $y_t\lt y_t+\epsilon$ and 
    $\frac{x^2}{y_t+\epsilon}=\frac{y_t^2}{y_t+\epsilon}\lt\frac{y_t^2}{y_t}=y_t$. 
    Then we see $\frac{x}{y_t+\epsilon} \lt y_t \lt y_t+\epsilon$. 
    Similar logic applies for $y_t-\epsilon$. 
  + So can instead solve the fixed-point problem 
  $y\mapsto \frac{1}{2} (y + x/y)$. 
+ [Fixed-point iteration in Scheme](./Book_Examples_Source/C1_3_3b.scm).
```scheme
(define (fixed-point f first-guess) 
  (define (try guess) 
    (let 
      ((next-guess (f guess))) 
      (if 
        (close-enough guess next-guess) 
        next-guess 
        (try next-guess)))) 
  (try first-guess))
```
+ *Average damping*: approach of averaging successinve approximations to a 
solution. Often aids convergence in fixed-point problems. 
+ *Continued fraction*: expression of the from:

$$
f=\frac{N_1}{D_1 + \frac{N_2}{D_2 + \frac{N_3}{D_3 + \dots}}}
$$

  + *k-term finite continued fraction*: 

$$
\frac{N_1}{D_1 + \frac{N_2}{\ddots + \frac{N_k}{D_k}}}
$$


### 1.3.4 Procedures as Returned Values

+ Expressive power increases when procedures can be passed as arguments. 
+ Expressive power increases even further when returned values can be 
procedures. 
+ *Average damping* as a general techniques: for function $f$, consider 
new function $g$ where $g(x) = \mathrm{average}(f(x),x)$. Concept 
can be used to define a procedure, whose return value is also a 
procedure:
  ```scheme
  (define (average-damp f) 
    (lambda (x) (average x (f x))))
  ```
  + i.e. $\mathrm{average\,damp}(f, x) = \mathrm{average}(f(x), x)$
+ [Square root calculation using fixed-point search, average damping and 
(lambda) function $y\mapsto x/y$](Book_Examples_Source/C1_3_4a.scm).
  ```scheme
  (define (sq-root x) 
    (fixed-point (average-damp (lambda (y) (/ x y))) 1.0))
  ```
  + Readily applicable to cube roots. 
  ```scheme
  (define (cube-root x) 
    (fixed-point (average-damp (lambda (y) (/ x (square y)))) 1.0))
  ```

#### Newton's method

+ *Newton's method*: if $x\mapsto g(x)$ is differentiable, then a 
solution to $g(x) = 0$ is a fixed point of the function 
$x\mapsto f(x)$, where:

$$
f(x)=x-\frac{g(x)}{\mathrm{D}g(x)}
$$

  + where: 

$$
\mathrm{D}g(x) = \frac{\mathrm{d}g}{\mathrm{d}x}(x)
$$

  + For many $g$ and sufficiently good intial guesses of $x$, solution 
  is converged on quickly. 
+ Recall for for small number $\mathrm{d}x$ as $\mathrm{d}x\to 0$, 
the *derivative* $\mathrm{D}g$ of $g$ is the function:

$$ 
\mathrm{D}g(x)=\frac{g(x+\mathrm{d}x)-g(x)}{\mathrm{d}x} 
$$

  + [Derivation as a scheme procedure](.Book_Examples_Source/C1_3_4b.scm).
  + Derivative allows Newton's method to be expressed as a fixed-point 
  process. 
  + Newton's method can find square roots $y^2=x$ but solving $0=y^2-x$. 
```scheme
(define (deriv g dx) 
  (lambda (x) (/ (- (g (+ x dx)) (g x)) dx)))

(define dx 0.00001)

(define (newton-transform g) 
  (lambda (x) (- x (/ (g x) ((deriv g dx) x)))))

(define (newton-method f first-guess) 
  (fixed-point (newton-transform f) first-guess))

(define (sq-root x) 
  (newton-method (lambda (y) (- (square y) x)) 1.0))
```

#### Abstractions and first-class procedures

+ Square root methods seen were ways to frame the problem as fixed-point 
problems. 
+ [Idea can be generalized](./Book_Examples_Source/C1_3_4c.scm), where 
functions are transformed into fixed-point problems then solved. 
```scheme
(define (fixed-point-of-transform g transform guess) 
  (fixed-point (transform g) guess))
```
+ Overall, see that:
  + Abstracting with compound procedures allow expression of general 
  computing methods as explicit elements.
  + Higher-order procedures allow manipulation of general methods into 
  further abstractions. 
+ Watch for opportunities to identify underlying abstractions in programs 
and build more powerful abstractions from them. 
  + Abstraction level should be chosen carefully. 
+ *First-class* status elements: elements in programming languages with fewest 
restrictions on their manipulation. First-class "rights and privilleges": 
  + May be named by variables. 
  + May be passed as arguments to procedures. 
  + May be returned as results of procedures. 
  + May be included in data structures. 
+ Lisp awards procedures full-class status. 
  + Challenging to implement: procedures' free variables must have reserved 
  storage even when not executing, if procedures are first-class. 
  + Enormous expressive power. 
+ *Composition* of function $f$ after $g$ is defined to be the mapping 
$x\mapsto f(g(x))$. 
+ *Smoothing* of function $f$ given some small $\mathrm{d}x$: the 
*smoothed* $f_s$: 

$$
f_s(x) = \mathrm{average}(f(x-\mathrm{d}x),f(x),f(x+\mathrm{d}x))
$$

+ *$n$-fold smoothed function*: repeatedly smoothing a function $n$ times. 
+ *Iterative improvement*: in trying to seek a solution, make an initial 
guess, see if the guess is close enough to be the solution, if true then 
use the guess, if not then improve the guess based on the current result, and 
continue the process until satisfactory.
